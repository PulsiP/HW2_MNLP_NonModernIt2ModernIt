{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45e01f8a",
      "metadata": {
        "id": "45e01f8a"
      },
      "source": [
        "## 1. Importing Libraries and Setup Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7289f7",
      "metadata": {
        "id": "fb7289f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc95fb1d-04c7-4326-85c8-6cbf7628af73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c395c6",
      "metadata": {
        "id": "23c395c6"
      },
      "outputs": [],
      "source": [
        "def get_prompt(prompt_name:str) -> str:\n",
        "    \"\"\"\n",
        "    Set the prompt the model we'll see as input.\n",
        "    We defined three types of prompts\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    - prompt_name: represents the prompt of a page, it must be a string\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    prompt = \"\"\n",
        "    match prompt_name:\n",
        "        case \"in_context\":\n",
        "            prompt = \"\"\"Sei un esperto di lingua italiana. Il tuo compito è modernizzare testi antichi in italiano contemporaneo. Di seguito alcuni esempi:\n",
        "\n",
        "            Testo antico: quella guerra ben fatta l' opera perché etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno\n",
        "            Traduzione moderna: Quella guerra fu ben condotta per raggiungere il suo scopo. Dall'altra parte, Aiace era un cavaliere leale e valoroso nelle armi, di grande statura, ma non molto saggio.\n",
        "\n",
        "            Testo antico: crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi.\n",
        "            Traduzione moderna: È crudele e si vendica di ogni colpa, come stabilisce la legge, e non perdona alcun cavaliere che commetta un errore.\n",
        "\n",
        "            Testo antico: Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.\n",
        "            Traduzione moderna: Ponzio Aufidiano, cavaliere romano, non era dotato di un coraggio superiore.\n",
        "\n",
        "            Testo antico: Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrò più i fati.\n",
        "            Traduzione moderna: Se questo è il volere di tutti, e se i tempi richiedono Pompeo come guida e non come compagno, allora non tratterrò oltre il destino.\n",
        "\n",
        "            Testo antico: Officio di questa arte pare che sia dicere appostatamente per fare credere, fine è far credere per lo dire.\n",
        "            Traduzione moderna: Il compito di quest’arte sembra essere quello di parlare in modo studiato per convincere; il fine è dunque persuadere attraverso le parole.\n",
        "\n",
        "            Perfavore non includere nella risposta il testo precedente, ma soltanto la traduzione.\n",
        "            Traduci la frase da italiano antico a quello moderno, scrivendo solo ed ESCLUSIVAMENTE la risposta.\n",
        "            Testo antico: {sentence}\n",
        "            Traduzione moderna:\n",
        "            \"\"\"\n",
        "        case \"language_expert\":\n",
        "            prompt = \"\"\"Sei un esperto di lingua italiana. Il tuo compito è modernizzare testi antichi in italiano contemporaneo.\n",
        "            Perfavore non includere nella risposta il testo precedente, ma soltanto la traduzione.\n",
        "            Traduci la seguente frase da italiano antico a quello moderno, scrivendo solo ed ESCLUSIVAMENTE la risposta.\n",
        "            Testo antico: {sentence}\n",
        "            Traduzione moderna:\n",
        "            \"\"\"\n",
        "        case \"with_rules\":\n",
        "            prompt = \"\"\"Traduci e modernizza una frase in italiano contemporaneo e comprensibile, data una frase in italiano Antico.\n",
        "            Fai affidamento in TUTTA la CONOSCENZA POSSEDUTA nel tradurre e modernizzare frasi da Italiano Antico a Italiano Moderno.\n",
        "\n",
        "            Ecco un esempio di traduzione:\n",
        "              Testo antico: Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.\n",
        "              Traduzione moderna: Ponzio Aufidiano, cavaliere romano, non era dotato di un coraggio superiore.\n",
        "\n",
        "            Devi seguire le seguenti regole:\n",
        "                1. La frase prodotta deve essere comprensibile\n",
        "                2. La frase prodotta deve avere un senso\n",
        "                3. Ristruttura la frase, in modo che sia più contemporanea possibile\n",
        "                4. Tutte le parole devono essere in italiano contemporaneo, escludendo nomi o luoghi\n",
        "\n",
        "            Perfavore restuituisci SOLO ED ESCLUSIVAMENTE la la frase tradotta e modernizzata.\n",
        "            Testo antico: {sentence}\n",
        "            Traduzione moderna:\n",
        "            \"\"\"\n",
        "        case _:\n",
        "            raise Exception(\"Unexpected name\")\n",
        "            exit(1)\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Option for the Quantized version of the model"
      ],
      "metadata": {
        "id": "zJJSxMl1x0qR"
      },
      "id": "zJJSxMl1x0qR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39aa253",
      "metadata": {
        "id": "d39aa253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be84539-4065-4269-aeba-2d912a290591"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sei un esperto di lingua italiana. Il tuo compito è modernizzare testi antichi in italiano contemporaneo. Di seguito alcuni esempi:\\n\\n            Testo antico: quella guerra ben fatta l' opera perché etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno\\n            Traduzione moderna: Quella guerra fu ben condotta per raggiungere il suo scopo. Dall'altra parte, Aiace era un cavaliere leale e valoroso nelle armi, di grande statura, ma non molto saggio.\\n\\n            Testo antico: crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi.\\n            Traduzione moderna: È crudele e si vendica di ogni colpa, come stabilisce la legge, e non perdona alcun cavaliere che commetta un errore.\\n\\n            Testo antico: Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.\\n            Traduzione moderna: Ponzio Aufidiano, cavaliere romano, non era dotato di un coraggio superiore.\\n\\n            Testo antico: Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrò più i fati.\\n            Traduzione moderna: Se questo è il volere di tutti, e se i tempi richiedono Pompeo come guida e non come compagno, allora non tratterrò oltre il destino.\\n\\n            Testo antico: Officio di questa arte pare che sia dicere appostatamente per fare credere, fine è far credere per lo dire.\\n            Traduzione moderna: Il compito di quest’arte sembra essere quello di parlare in modo studiato per convincere; il fine è dunque persuadere attraverso le parole.\\n\\n            Perfavore non includere nella risposta il testo precedente, ma soltanto la traduzione.\\n            Traduci la frase da italiano antico a quello moderno, scrivendo solo ed ESCLUSIVAMENTE la risposta.\\n            Testo antico: {sentence}\\n            Traduzione moderna:\\n            \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "quantize_model = False\n",
        "\n",
        "\"\"\"\n",
        "You can choose from:\n",
        "    1. \"in_context\"\n",
        "    2. \"language_expert\"\n",
        "    3. \"with_rules\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "prompt = get_prompt(\"in_context\")\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba365847",
      "metadata": {
        "id": "ba365847"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quant_config = None\n",
        "if quantize_model:\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=\"float16\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b86ca101",
      "metadata": {
        "id": "b86ca101"
      },
      "source": [
        "## 3. Loading Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "K9HfGlO-5NjT",
      "metadata": {
        "id": "K9HfGlO-5NjT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"test.csv\", sep=\";\")\n",
        "df = df.rename(columns={\"Sentence\": \"source\", \"Traductions\": \"target\"})\n",
        "\n",
        "dataset = Dataset.from_pandas(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7oRcr5Ui72JK",
      "metadata": {
        "id": "7oRcr5Ui72JK"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08c5091",
      "metadata": {
        "id": "c08c5091"
      },
      "source": [
        "## 4. Define Quantization and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1279993d",
      "metadata": {
        "id": "1279993d"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/flan-t5-XL\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
        "                                              device_map=\"auto\",\n",
        "                                              quantization_config=quant_config)\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3308d17a",
      "metadata": {
        "id": "3308d17a"
      },
      "source": [
        "## 5. Prometheus Evaluator\n",
        "Since the PROMETHEUS model used with VLLM has 7 billion parameters, we attempted to load it using\n",
        "the Hugging Face Transformers library and then quantize it in order to reduce memory usage and improve inference efficiency.   \n",
        "To address this:\n",
        "\n",
        " - We used the Hugging Face Transformers library to load the model, as it provides a standardized interface for accessing pretrained weights and integrating them into existing pipelines.\n",
        "\n",
        " - We then applied quantization, a common technique that reduces the numerical precision of the model weights , with the goal of:\n",
        "\n",
        "        - Lowering memory consumption\n",
        "\n",
        "        - Speeding up inference\n",
        "\n",
        "        - Maintaining reasonable accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tHe_frZs5WU-",
      "metadata": {
        "id": "tHe_frZs5WU-"
      },
      "outputs": [],
      "source": [
        "from prometheus import PrometheusEval_AtM\n",
        "\n",
        "evaluator = PrometheusEval_AtM(quantized = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Translating phase"
      ],
      "metadata": {
        "id": "mrk98vtOyNc-"
      },
      "id": "mrk98vtOyNc-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bkXSKmVg6UUx",
      "metadata": {
        "id": "bkXSKmVg6UUx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "i = 1\n",
        "\n",
        "source_sentences = []\n",
        "predicted_sentences = []\n",
        "gold_sentences = [] #annoted by hand\n",
        "prometheus_score = []\n",
        "df_col = [\"source_sentences\", \"gold_sentences\",\"predicted_sentences\", \"prometheus_score\",\"GPT_score\",\"user_score\"]\n",
        "\n",
        "print(len(dataset))\n",
        "for sample in dataset:\n",
        "\n",
        "\n",
        "    input_sentence = sample[\"source\"]\n",
        "    target_sentence = sample[\"target\"]\n",
        "\n",
        "    author = sample[\"Author\"]\n",
        "    date = sample[\"Date\"]\n",
        "    region = sample[\"Region\"]\n",
        "\n",
        "    user_prompt = prompt.format(sentence=input_sentence)\n",
        "\n",
        "    translation =  pipe(user_prompt, max_new_tokens=512)[0]['generated_text']\n",
        "\n",
        "    evaluation = evaluator.getEvaluation(input_sentence, translation, target_sentence)\n",
        "\n",
        "\n",
        "    match_ = re.search(r'\\[RESULT\\]\\s*(\\d)', evaluation)\n",
        "    if match_:\n",
        "      result = int(match_.group(1))\n",
        "\n",
        "    else:\n",
        "      result = 0\n",
        "\n",
        "    source_sentences.append(input_sentence)\n",
        "    predicted_sentences.append(translation)\n",
        "    gold_sentences.append(target_sentence)\n",
        "    prometheus_score.append(result)\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Sentence {i}\")\n",
        "    print(f\"\\tItaliano Arcaico            -> {input_sentence}\")\n",
        "    print(f\"\\tItaliano moderno            -> {translation}\")\n",
        "    print(f\"\\tGOLD LABEL                  -> {target_sentence}\")\n",
        "    print(f\"\\tPROMETHEUS EVALUATION       -> {result}\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "    i+=1\n",
        "\n",
        "z = [0 for _ in range(len(dataset))]\n",
        "GPT_score,user_score = z,z\n",
        "df = pd.DataFrame(list(zip(source_sentences, gold_sentences,predicted_sentences,prometheus_score,GPT_score,user_score)), columns=df_col)\n",
        "\n",
        "df.to_csv(\"test_results_base.csv\", sep=\";\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}