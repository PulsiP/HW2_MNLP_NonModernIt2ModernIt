{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzt5-YHDvRAn"
      },
      "source": [
        "## 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNuqXcD0oQfE"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import pipeline,AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "login(token=\"your_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5V44EFtvcsW"
      },
      "source": [
        "## 2. Importing the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyd_wVApoZRj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"test.csv\", sep=\";\")\n",
        "df = df.rename(columns={\"Sentence\": \"source\", \"Traductions\": \"target\"})\n",
        "\n",
        "dataset = Dataset.from_pandas(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG6ceZ94vhAe"
      },
      "source": [
        "## 3. Importing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYmEGyRbqDmP"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
        "#model_name = \"sapienzanlp/Minerva-7B-base-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                             device_map=\"auto\")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDnCBzIEvker"
      },
      "source": [
        "## 4. Prometheus Evaluator\n",
        "Since the PROMETHEUS model used with VLLM has 7 billion parameters, we attempted to load it using\n",
        "the Hugging Face Transformers library and then quantize it in order to reduce memory usage and improve inference efficiency.   \n",
        "To address this:\n",
        "\n",
        " - We used the Hugging Face Transformers library to load the model, as it provides a standardized interface for accessing pretrained weights and integrating them into existing pipelines.\n",
        "\n",
        " - We then applied quantization, a common technique that reduces the numerical precision of the model weights , with the goal of:\n",
        "\n",
        "        - Lowering memory consumption\n",
        "\n",
        "        - Speeding up inference\n",
        "\n",
        "        - Maintaining reasonable accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv9jWoyMxDaL"
      },
      "outputs": [],
      "source": [
        "from prometheus import PrometheusEval_AtM\n",
        "\n",
        "evaluator = PrometheusEval_AtM(quantized = True, device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK81y2Olv5Uo"
      },
      "source": [
        "## 5. Examples for In-Context_Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUq8caDO4qwT"
      },
      "outputs": [],
      "source": [
        "few_shot_examples =\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Testo antico: \"quella guerra ben fatta l' opera perché etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno\"\n",
        "Traduzione moderna: \"Quella guerra fu ben condotta per raggiungere il suo scopo. Dall'altra parte, Aiace era un cavaliere leale e valoroso nelle armi, di grande statura, ma non molto saggio.\"\n",
        "\n",
        "Testo antico: crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi.\"\n",
        "Traduzione moderna: \"È crudele e si vendica di ogni colpa, come stabilisce la legge, e non perdona alcun cavaliere che commetta un errore.\"\n",
        "\n",
        "Testo antico: \"Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.\"\n",
        "Traduzione moderna: \"Ponzio Aufidiano, cavaliere romano, non era dotato di un coraggio superiore.\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxQohCVuxXAK"
      },
      "source": [
        "## 6. Translating using In-Context-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rojp1yxhqgmo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "i = 1\n",
        "\n",
        "source_sentences = []\n",
        "predicted_sentences = []\n",
        "gold_sentences = [] #annoted by hand\n",
        "prometheus_score = []\n",
        "df_col = [\"source_sentences\", \"gold_sentences\",\"predicted_sentences\", \"prometheus_score\",\"GPT_score\",\"user_score\"]\n",
        "\n",
        "i = 1\n",
        "for sample in dataset:\n",
        "\n",
        "\n",
        "    input_sentence = sample[\"source\"]\n",
        "    target_sentence = sample[\"target\"]\n",
        "\n",
        "    author = sample[\"Author\"]\n",
        "    date = sample[\"Date\"]\n",
        "    region = sample[\"Region\"]\n",
        "\n",
        "    #user_prompt = prompt.format(author=author, date=date, region=region, input_sentence=input_sentence)\n",
        "    user_prompt = f\"\"\"\n",
        "\n",
        "    {few_shot_examples}\n",
        "\n",
        "    Testo antico: \"{input_sentence}\"\n",
        "    Traduzione moderna:\n",
        "    \"\"\"\n",
        "\n",
        "    output = pipe(user_prompt.strip(), max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "    if \"Traduzione moderna:\" in output:\n",
        "        translation = output.split(\"Traduzione moderna:\")[-1].strip()\n",
        "    else:\n",
        "        translation = output.strip()\n",
        "    translation = translation.split('\"')[1]\n",
        "\n",
        "    evaluation = evaluator.getEvaluation(input_sentence, translation, target_sentence)\n",
        "    match_ = re.search(r'\\[RESULT\\]\\s*(\\d)', evaluation)\n",
        "    if match_:\n",
        "      result = int(match_.group(1))\n",
        "\n",
        "    else:\n",
        "      result = 0\n",
        "\n",
        "    source_sentences.append(input_sentence)\n",
        "    predicted_sentences.append(translation)\n",
        "    gold_sentences.append(target_sentence)\n",
        "    prometheus_score.append(result)\n",
        "\n",
        "    print(f\"Sentence {i}\")\n",
        "    print(f\"\\tItaliano Arcaico -> {input_sentence}\")\n",
        "    print(f\"\\tItaliano moderno -> {translation}\")\n",
        "    print(f\"\\tGOLD LABEL       -> {target_sentence}\")\n",
        "    print(f\"\\tEVALUATION       -> {result}\")\n",
        "    print(f\"-----------------------------------------\")\n",
        "    i+=1\n",
        "\n",
        "z = [0 for _ in range(len(dataset))]\n",
        "GPT_score,user_score = z,z\n",
        "df = pd.DataFrame(list(zip(source_sentences, gold_sentences,predicted_sentences,prometheus_score,GPT_score,user_score)), columns=df_col)\n",
        "\n",
        "df.to_csv(\"test_results_base.csv\", sep=\";\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
